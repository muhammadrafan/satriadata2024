{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Satria Data 2024"
      ],
      "metadata": {
        "id": "17cX2Ahjfq-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Data"
      ],
      "metadata": {
        "id": "S74EVYgVkWeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim\n"
      ],
      "metadata": {
        "id": "o6Vais5A1-PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Sastrawi\n"
      ],
      "metadata": {
        "id": "US8lAg-4djGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade catboost\n"
      ],
      "metadata": {
        "id": "JxKjcfdareGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "id": "GfW4qjE3sSIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "H-ZOeyEXxSAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtSrll5c-te"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, balanced_accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "z04qodvQdv9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))"
      ],
      "metadata": {
        "id": "zl8KU9y8xQMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "vMnfGLeXRkSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data =  pd.read_csv('/content/dataset_penyisihan_bdc_2024.csv',delimiter=\";\")\n",
        "unlabeled_data = pd.read_csv('/content/dataset_unlabeled_penyisihan_bdc_2024.csv',delimiter=\";\")"
      ],
      "metadata": {
        "id": "OR4CMh2GfvuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data"
      ],
      "metadata": {
        "id": "mqhLrnzZf4SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "nVrymDRGhCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daftar kata yang akan dihapus\n",
        "keywords_to_remove = ['anies', 'ganjar', 'baswedan', 'mahfud', 'md', 'prabowo', 'gibran']\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Hapus teks dalam kurung siku beserta kurung sikunya\n",
        "    # Hapus RT atau RE di awal teks\n",
        "    text = re.sub(r'^(RT|RE)\\s+', '', text)\n",
        "    # Hapus teks di belakang karakter @\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    # Hapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Hapus karakter khusus dan angka\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Hapus karakter ASCII yang tidak diinginkan\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # Hapus tanda baca\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Hapus spasi ekstra\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\byg\\b', '', text, flags=re.IGNORECASE)  # Hapus kata RE\n",
        "    # Pisahkan teks menjadi kata-kata individu\n",
        "    words = text.split()\n",
        "    # Hapus kata-kata tertentu\n",
        "    filtered_words = [word for word in words if word.lower() not in keywords_to_remove]\n",
        "    # Hapus kata imbuhan dan kata-kata stop_words jika ada\n",
        "    filtered_words = [word for word in filtered_words if word.lower() not in stop_words]\n",
        "    # Gabungkan kembali kata-kata yang telah difilter\n",
        "    text = ' '.join(filtered_words)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "wWRJhcZrgEqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terapkan fungsi clean_text ke kolom teks\n",
        "labeled_data['cleaned_text'] = labeled_data['text'].apply(clean_text)\n",
        "unlabeled_data['cleaned_text'] = unlabeled_data['Text'].apply(clean_text)\n",
        "\n",
        "# Tampilkan beberapa baris untuk memastikan teks telah dibersihkan\n",
        "print(labeled_data[['text', 'cleaned_text']])\n",
        "print(unlabeled_data[['Text', 'cleaned_text']])"
      ],
      "metadata": {
        "id": "mXKuNrpHhFqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus duplikat di labeled_data\n",
        "labeled_data_cleaned = labeled_data.drop_duplicates()\n",
        "\n",
        "# Menghapus duplikat di unlabeled_data\n",
        "unlabeled_data_cleaned = unlabeled_data.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "G5tQUWf2hmgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pisahkan text berdasarkan label"
      ],
      "metadata": {
        "id": "javY4T0ukn-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan teks berdasarkan label\n",
        "texts_by_label = labeled_data.groupby('label')['cleaned_text'].apply(lambda x: ' '.join(x)).reset_index()"
      ],
      "metadata": {
        "id": "3b1f4D1jibQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(text, n=10):\n",
        "    words = text.split()\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts.most_common(n)\n",
        "\n",
        "# Hitung frekuensi kata untuk setiap label\n",
        "texts_by_label['top_words'] = texts_by_label['cleaned_text'].apply(lambda x: get_top_n_words(x, n=10))\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(texts_by_label[['label', 'top_words']])\n"
      ],
      "metadata": {
        "id": "aUwlyptvkvDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi frekuensi kata teratas untuk setiap label\n",
        "def plot_top_words(top_words, label):\n",
        "    words, counts = zip(*top_words)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(words))\n",
        "    plt.title(f\"Top Words in {label}\")\n",
        "    plt.xlabel(\"Counts\")\n",
        "    plt.ylabel(\"Words\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot untuk setiap label\n",
        "for index, row in texts_by_label.iterrows():\n",
        "    plot_top_words(row['top_words'], row['label'])\n"
      ],
      "metadata": {
        "id": "GWyG5eAhkyZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitur Ekstraksi"
      ],
      "metadata": {
        "id": "12HCKWqNmwy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "metadata": {
        "id": "hrljqRjM0HyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "from transformers.data.processors.utils import InputExample\n",
        "\n",
        "# Updated keyword dictionary\n",
        "keyword_dict = {\n",
        "    'Ekonomi': ['jnk', 'ekonomi', 'tani'],\n",
        "    'Demografi': ['kalangan muda', 'kalangan'],\n",
        "    'Geografi': ['pulau', 'jakarta', 'kota'],\n",
        "    'Ideologi': ['dasar negara', 'dasar', 'negara', 'konstitusi'],\n",
        "    'Pertahanan dan Keamanan': ['tahan', 'tni', 'serang', 'akun', 'masuk', 'rawan'],\n",
        "    'Politik': ['ganjarmahfudpilihanumat', 'dukung'],\n",
        "    'Sumber Daya Alam': ['air', 'tani', 'indosentris']\n",
        "}\n",
        "\n",
        "# Fungsi untuk mengonversi data teks menjadi fitur LDA\n",
        "def convert_text_to_lda_features(texts, keyword_dict):\n",
        "    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    lda_features = lda.transform(X)\n",
        "\n",
        "    # Tambahkan fitur kata kunci\n",
        "    keyword_features = np.zeros((len(texts), len(keyword_dict)))\n",
        "    for i, text in enumerate(texts):\n",
        "        for j, (category, keywords) in enumerate(keyword_dict.items()):\n",
        "            if any(keyword in text for keyword in keywords):\n",
        "                keyword_features[i, j] = 1\n",
        "\n",
        "    combined_features = np.hstack((lda_features, keyword_features))\n",
        "    return combined_features\n",
        "\n",
        "# Fungsi untuk mengonversi data menjadi InputExample untuk BERT\n",
        "def convert_data_to_examples(texts, labels):\n",
        "    return [InputExample(guid=None, text_a=text, text_b=None, label=label) for text, label in zip(texts, labels)]\n",
        "\n",
        "# Fungsi untuk mengonversi InputExample menjadi tf.data.Dataset\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, lda_features, max_length=128):\n",
        "    features = []\n",
        "\n",
        "    for e, lda_feat in zip(examples, lda_features):\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
        "\n",
        "        # Konversi fitur LDA menjadi tf.Tensor\n",
        "        lda_tensor = tf.convert_to_tensor(lda_feat, dtype=tf.float32)\n",
        "\n",
        "        # Gabungkan input_ids, token_type_ids, attention_mask, dan lda_tensor\n",
        "        combined_input = {\n",
        "            \"input_ids\": tf.convert_to_tensor(input_ids, dtype=tf.int32),\n",
        "            \"token_type_ids\": tf.convert_to_tensor(token_type_ids, dtype=tf.int32),\n",
        "            \"attention_mask\": tf.convert_to_tensor(attention_mask, dtype=tf.int32),\n",
        "            \"lda_features\": lda_tensor\n",
        "        }\n",
        "\n",
        "        features.append((combined_input, e.label))\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield f\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            {\n",
        "                \"input_ids\": tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
        "                \"token_type_ids\": tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
        "                \"attention_mask\": tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
        "                \"lda_features\": tf.TensorSpec(shape=(20,), dtype=tf.float32)  # 10 LDA + 10 keyword categories\n",
        "            },\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Convert texts to LDA features\n",
        "train_lda_features = convert_text_to_lda_features(X_train, keyword_dict)\n",
        "test_lda_features = convert_text_to_lda_features(X_test, keyword_dict)\n",
        "\n",
        "# Convert LDA features to InputExamples\n",
        "train_InputExamples = convert_data_to_examples(X_train, y_train)\n",
        "validation_InputExamples = convert_data_to_examples(X_test, y_test)\n",
        "\n",
        "# Convert examples to tf.data.Dataset with LDA features\n",
        "train_data = convert_examples_to_tf_dataset(train_InputExamples, tokenizer, train_lda_features)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(validation_InputExamples, tokenizer, test_lda_features)\n",
        "validation_data = validation_data.batch(32)\n",
        "\n",
        "# Custom model combining BERT and LDA features\n",
        "class CustomBertModel(tf.keras.Model):\n",
        "    def __init__(self, bert_model, num_labels):\n",
        "        super(CustomBertModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dense = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.lda_dense = tf.keras.layers.Dense(768, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_inputs = {k: v for k, v in inputs.items() if k != 'lda_features'}\n",
        "        lda_features = inputs['lda_features']\n",
        "\n",
        "        bert_output = self.bert(bert_inputs, training=training)[1]\n",
        "        lda_output = self.lda_dense(lda_features)\n",
        "\n",
        "        combined_output = tf.concat([bert_output, lda_output], axis=-1)\n",
        "        combined_output = self.dropout(combined_output, training=training)\n",
        "        return self.dense(combined_output)\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Create custom model\n",
        "custom_model = CustomBertModel(bert_model, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "history = custom_model.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "\n",
        "# Evaluate the model\n",
        "preds = custom_model.predict(validation_data)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "# Decode labels\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test_decoded, y_pred_decoded))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_decoded, y_pred_decoded))\n"
      ],
      "metadata": {
        "id": "972zQdy6o8DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel,TFBertForSequenceClassification\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Fungsi untuk mengonversi data teks menjadi fitur LDA\n",
        "def convert_text_to_lda_features(texts):\n",
        "    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    return lda.transform(X)\n",
        "\n",
        "# Fungsi untuk mengonversi data menjadi InputExample untuk BERT\n",
        "def convert_data_to_examples(texts, labels):\n",
        "    return [InputExample(guid=None, text_a=text, text_b=None, label=label) for text, label in zip(texts, labels)]\n",
        "\n",
        "# Fungsi untuk mengonversi InputExample menjadi tf.data.Dataset\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, lda_features, max_length=128):\n",
        "    features = []\n",
        "\n",
        "    for e, lda_feat in zip(examples, lda_features):\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
        "\n",
        "        # Konversi fitur LDA menjadi tf.Tensor\n",
        "        lda_tensor = tf.convert_to_tensor(lda_feat, dtype=tf.float32)\n",
        "\n",
        "        # Gabungkan input_ids, token_type_ids, attention_mask, dan lda_tensor\n",
        "        combined_input = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"lda_features\": lda_tensor\n",
        "        }\n",
        "\n",
        "        features.append((combined_input, e.label))\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield f\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            {\n",
        "                \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "                \"token_type_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "                \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "                \"lda_features\": tf.TensorSpec(shape=(10,), dtype=tf.float32)\n",
        "            },\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Convert texts to LDA features\n",
        "train_lda_features = convert_text_to_lda_features(X_train)\n",
        "test_lda_features = convert_text_to_lda_features(X_test)\n",
        "\n",
        "# Convert LDA features to InputExamples\n",
        "train_InputExamples = convert_data_to_examples(X_train, y_train)\n",
        "validation_InputExamples = convert_data_to_examples(X_test, y_test)\n",
        "\n",
        "# Convert examples to tf.data.Dataset with LDA features\n",
        "train_data = convert_examples_to_tf_dataset(train_InputExamples, tokenizer, train_lda_features)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(validation_InputExamples, tokenizer, test_lda_features)\n",
        "validation_data = validation_data.batch(32)\n",
        "\n",
        "# Custom model combining BERT and LDA features\n",
        "class CustomBertModel(tf.keras.Model):\n",
        "    def __init__(self, bert_model, num_labels):\n",
        "        super(CustomBertModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dense = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.lda_dense = tf.keras.layers.Dense(768, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_inputs = {k: v for k, v in inputs.items() if k != 'lda_features'}\n",
        "        lda_features = inputs['lda_features']\n",
        "\n",
        "        bert_output = self.bert(bert_inputs, training=training)[1]\n",
        "        lda_output = self.lda_dense(lda_features)\n",
        "\n",
        "        combined_output = tf.concat([bert_output, lda_output], axis=-1)\n",
        "        combined_output = self.dropout(combined_output, training=training)\n",
        "        return self.dense(combined_output)\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Create custom model\n",
        "custom_model = CustomBertModel(bert_model, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "history = custom_model.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "\n",
        "# Evaluate the model\n",
        "preds = custom_model.predict(validation_data)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "# Decode labels\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test_decoded, y_pred_decoded))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_decoded, y_pred_decoded))"
      ],
      "metadata": {
        "id": "LlxerFUPtq2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Verifikasi bahwa semua label telah dikonversi ke numerik\n",
        "print(\"Encoded labels:\", set(y_encoded))\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Fungsi untuk membuat data input untuk BERT\n",
        "def convert_data_to_examples(texts, labels):\n",
        "    return [InputExample(guid=None, text_a=text, text_b=None, label=label) for text, label in zip(texts, labels)]\n",
        "\n",
        "# Konversi data ke InputExample\n",
        "train_InputExamples = convert_data_to_examples(X_train, y_train)\n",
        "validation_InputExamples = convert_data_to_examples(X_test, y_test)\n",
        "\n",
        "# Fungsi untuk membuat InputFeatures\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = []\n",
        "\n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label)\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.int32,\n",
        "                \"attention_mask\": tf.int32,\n",
        "                \"token_type_ids\": tf.int32,\n",
        "            },\n",
        "            tf.int64,\n",
        "        ),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Konversi data ke tf.data.Dataset\n",
        "train_data = convert_examples_to_tf_dataset(train_InputExamples, tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(validation_InputExamples, tokenizer)\n",
        "validation_data = validation_data.batch(32)\n",
        "\n",
        "# Load model BERT untuk klasifikasi\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"indobenchmark/indobert-large-p2\", num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Latih model\n",
        "model.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "\n",
        "# Evaluasi model\n",
        "preds = model.predict(validation_data)\n",
        "y_pred = np.argmax(preds.logits, axis=1)\n",
        "\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "NR4odyTlQrkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi teks ke format input untuk BERT\n",
        "unlabeled_examples = convert_data_to_examples(unlabeled_data['cleaned_text'], [0]*len(unlabeled_data))\n",
        "unlabeled_data_tf = convert_examples_to_tf_dataset(unlabeled_examples, tokenizer)\n",
        "unlabeled_data_tf = unlabeled_data_tf.batch(32)\n",
        "\n",
        "# Prediksi label untuk data tidak berlabel\n",
        "unlabeled_preds = model.predict(unlabeled_data_tf)\n",
        "unlabeled_y_pred = np.argmax(unlabeled_preds.logits, axis=1)\n",
        "\n",
        "# Konversi label numerik kembali ke label asli\n",
        "unlabeled_labels = label_encoder.inverse_transform(unlabeled_y_pred)\n",
        "\n",
        "# Simpan hasil prediksi ke file CSV untuk submission\n",
        "submission = pd.DataFrame({'IDText': unlabeled_data['IDText'], 'Kelas': unlabeled_labels})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created successfully!\")"
      ],
      "metadata": {
        "id": "Jgb1aDydqjXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train FastText model\n",
        "sentences = [text.split() for text in X_train]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "def get_fasttext_embeddings(text, model):\n",
        "    words = text.split()\n",
        "    embeddings = [model.wv[word] for word in words if word in model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Compute FastText embeddings for training and test set\n",
        "X_train_fasttext = np.array([get_fasttext_embeddings(text, fasttext_model) for text in X_train])\n",
        "X_test_fasttext = np.array([get_fasttext_embeddings(text, fasttext_model) for text in X_test])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Apply LDA to get topic distributions\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "X_train_lda = lda.fit_transform(X_train_tfidf)\n",
        "X_test_lda = lda.transform(X_test_tfidf)\n",
        "\n",
        "# Load IndoBERT model from sentence-transformers\n",
        "model = SentenceTransformer(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Compute IndoBERT embeddings for training set\n",
        "train_sentence_embeddings = model.encode(X_train.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Compute IndoBERT embeddings for test set\n",
        "test_sentence_embeddings = model.encode(X_test.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Combine LDA features, IndoBERT embeddings, and FastText embeddings\n",
        "X_train_combined = np.hstack((X_train_lda, train_sentence_embeddings, X_train_fasttext))\n",
        "X_test_combined = np.hstack((X_test_lda, test_sentence_embeddings, X_test_fasttext))\n",
        "\n",
        "# Train CatBoost model using combined features\n",
        "model = CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', verbose=0)\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Predict on test set using combined features\n",
        "y_pred = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "kf7Z9vQH2JC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Apply LDA to get topic distributions\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "X_train_lda = lda.fit_transform(X_train_tfidf)\n",
        "X_test_lda = lda.transform(X_test_tfidf)\n",
        "\n",
        "# Load IndoBERT model from sentence-transformers\n",
        "model = SentenceTransformer(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Compute IndoBERT embeddings for training set\n",
        "train_sentence_embeddings = model.encode(X_train.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Compute IndoBERT embeddings for test set\n",
        "test_sentence_embeddings = model.encode(X_test.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Ensure IndoBERT embeddings are 2D by averaging word embeddings in each sentence\n",
        "train_sentence_embeddings_2d = np.array([np.mean(emb, axis=0) if len(emb.shape) == 2 else emb for emb in train_sentence_embeddings])\n",
        "test_sentence_embeddings_2d = np.array([np.mean(emb, axis=0) if len(emb.shape) == 2 else emb for emb in test_sentence_embeddings])\n",
        "\n",
        "# Check dimensions\n",
        "print(\"X_train_lda shape:\", X_train_lda.shape)\n",
        "print(\"train_sentence_embeddings_2d shape:\", train_sentence_embeddings_2d.shape)\n",
        "print(\"X_test_lda shape:\", X_test_lda.shape)\n",
        "print(\"test_sentence_embeddings_2d shape:\", test_sentence_embeddings_2d.shape)\n",
        "\n",
        "# Combine LDA features and IndoBERT embeddings\n",
        "X_train_combined = np.hstack((X_train_lda, train_sentence_embeddings_2d))\n",
        "X_test_combined = np.hstack((X_test_lda, test_sentence_embeddings_2d))\n",
        "\n",
        "# Train CatBoost model using combined features\n",
        "catboost_model = CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', verbose=0)\n",
        "catboost_model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Predict on test set using combined features\n",
        "y_pred = catboost_model.predict(X_test_combined)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ll8QyeOeyfrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']\n",
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Apply LDA to get topic distributions\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "X_train_lda = lda.fit_transform(X_train_tfidf)\n",
        "X_test_lda = lda.transform(X_test_tfidf)\n",
        "\n",
        "# Load IndoBERT model from sentence-transformers\n",
        "model = SentenceTransformer(\"indobenchmark/indobert-large-p2\")\n",
        "\n",
        "# Compute IndoBERT embeddings for training set\n",
        "train_sentence_embeddings = model.encode(X_train.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Compute IndoBERT embeddings for test set\n",
        "test_sentence_embeddings = model.encode(X_test.tolist(), show_progress_bar=True)\n",
        "\n",
        "# Combine LDA features and IndoBERT embeddings\n",
        "import numpy as np\n",
        "X_train_combined = np.hstack((X_train_lda, train_sentence_embeddings))\n",
        "X_test_combined = np.hstack((X_test_lda, test_sentence_embeddings))\n",
        "\n",
        "# Train CatBoost model using combined features\n",
        "model = CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', verbose=0)\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Predict on test set using combined features\n",
        "y_pred = model.predict(X_test_combined)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "DIVDIUcCwJI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words=stopwords.words('indonesian'))\n",
        "\n",
        "# Fit dan transform teks\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_by_label['cleaned_text'])\n",
        "\n",
        "# Membuat DataFrame dari hasil TF-IDF\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_df['label'] = texts_by_label['label']\n",
        "\n",
        "# Tampilkan fitur TF-IDF untuk setiap label\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "id": "tzQNfX0Hk6vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Apply LDA to get topic distributions\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "X_train_lda = lda.fit_transform(X_train_tfidf)\n",
        "X_test_lda = lda.transform(X_test_tfidf)"
      ],
      "metadata": {
        "id": "IWeJM5RGvrc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mencoba Model"
      ],
      "metadata": {
        "id": "zr7oPVXGpjre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan data menjadi fitur (X) dan label (y)\n",
        "X = labeled_data['cleaned_text']\n",
        "y = labeled_data['label']"
      ],
      "metadata": {
        "id": "wv-giOtPm_VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagi data menjadi set pelatihan dan set pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bAKj4_Yhpoa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer IndoBERT yang telah dituning ulang\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
        "model = AutoModel.from_pretrained(\"indobenchmark/indobert-large-p2\")"
      ],
      "metadata": {
        "id": "iLqEJGXZpqgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi teks\n",
        "X_train_tokenized = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "X_test_tokenized = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Konversi token menjadi teks\n",
        "X_train_text = tokenizer.batch_decode(X_train_tokenized['input_ids'], skip_special_tokens=True)\n",
        "X_test_text = tokenizer.batch_decode(X_test_tokenized['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "# Ekstraksi fitur dengan TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)"
      ],
      "metadata": {
        "id": "eBbrV7rLpsQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat pipeline dengan Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('clf', CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0:1', verbose=0))\n",
        "])"
      ],
      "metadata": {
        "id": "leSqEADKpxPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih model\n",
        "pipeline.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi pada set pengujian\n",
        "y_pred = pipeline.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluasi kinerja model dengan macro recall\n",
        "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print macro recall\n",
        "print(\"Macro Recall:\", macro_recall)"
      ],
      "metadata": {
        "id": "slR6fnNgp3TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1vvYm90YrahD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnJE4NqOp5pB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}